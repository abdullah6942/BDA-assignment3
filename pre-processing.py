# -*- coding: utf-8 -*-
"""Final_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VfieKXPoPNfqnRnznOvw08JYL9VlDUJL
"""

import pandas as pd
import json
import re

# Define a function to remove HTML tags from text
def remove_html_tags(text):
    if isinstance(text, str):
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    else:
        return text

# Define a function to remove URLs from text
def remove_urls(text):
    if isinstance(text, str):
        clean = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        return re.sub(clean, '', text)
    else:
        return text

# Processed chunk count
processed_chunks = 0

# Load JSON file in chunks
chunk_size = 10000  # Adjust chunk size as needed
with open('Sample_15.json', 'r') as file:
    for _ in range(processed_chunks * chunk_size):  # Skip rows if necessary
        next(file)

    for chunk in pd.read_json(file, lines=True, chunksize=chunk_size):
        # Remove unwanted columns
        unwanted_columns = ['imageURL', 'imageURL']
        chunk = chunk.drop(columns=unwanted_columns, errors='ignore')

        # Remove HTML tags and URLs from all fields
        for col in chunk.columns:
            chunk[col] = chunk[col].apply(remove_html_tags)
            chunk[col] = chunk[col].apply(remove_urls)

        # Update processed chunk count
        processed_chunks += 1

        # Perform your data processing here
        #print(chunk.head())  # For demonstration purposes only, remove this line in actual processing

        # Write cleaned chunk to file
        with open('cleaned.json', 'a') as cleaned_file:
            chunk.to_json(cleaned_file, orient='records', lines=True)

import pandas as pd

# Define chunk size
chunk_size = 10000

# Initialize processed chunks count
processed_chunks = 0

# Open the cleaned.json file for reading
with open('cleaned.json', 'r') as file:
    while True:
        # Read the next chunk of data
        chunk = pd.read_json(file, lines=True, chunksize=chunk_size)

        # Initialize a flag to check if there is any data in the chunk
        chunk_has_data = False

        # Process each chunk
        for data_chunk in chunk:
            chunk_has_data = True  # Set flag to True as there is data in the chunk

            # Drop columns with null values
            data_chunk = data_chunk.dropna(axis=1)

            # Drop rows with null values
            data_chunk = data_chunk.dropna()

            # Save the further cleaned data to "cleaned2.json"
            with open('cleaned2.json', 'a') as cleaned_file:
                data_chunk.to_json(cleaned_file, orient='records', lines=True)

        # If there is no more data, break the loop
        if not chunk_has_data:
            break

        # Update the processed chunk count
        processed_chunks += 1

import pandas as pd

# Define chunk size
chunk_size = 10000

# Initialize processed chunks count
processed_chunks = 0

# Open the cleaned2.json file for reading
with open('cleaned2.json', 'r') as file:
    while True:
        # Read the next chunk of data
        chunk = pd.read_json(file, lines=True, chunksize=chunk_size)

        # Initialize a flag to check if there is any data in the chunk
        chunk_has_data = False

        # Process each chunk
        for data_chunk in chunk:
            chunk_has_data = True  # Set flag to True as there is data in the chunk

            # Remove empty strings
            data_chunk = data_chunk.applymap(lambda x: None if isinstance(x, str) and x.strip() == '' else x)

            # Remove '\n' from each cell and drop the column if it contains '\n' for any row
            for col in data_chunk.columns:
                data_chunk[col] = data_chunk[col].apply(lambda x: None if isinstance(x, str) and '\n' in x else x)

            # Save the further cleaned data to "cleaned3.json"
            with open('cleaned3.json', 'a') as cleaned_file:
                data_chunk.to_json(cleaned_file, orient='records', lines=True)

        # If there is no more data, break the loop
        if not chunk_has_data:
            break

        # Update the processed chunk count
        processed_chunks += 1

import pandas as pd

# Define chunk size
chunk_size = 10000

# Initialize processed chunks count
processed_chunks = 0

# Open the cleaned3.json file for reading
with open('cleaned3.json', 'r') as file:
    while True:
        # Read the next chunk of data
        chunk = pd.read_json(file, lines=True, chunksize=chunk_size)

        # Initialize a flag to check if there is any data in the chunk
        chunk_has_data = False

        # Process each chunk
        for data_chunk in chunk:
            chunk_has_data = True  # Set flag to True as there is data in the chunk

            # Drop all columns except 'also_buy' and 'asin'
            columns_to_keep = ['also_buy', 'asin']
            data_chunk = data_chunk[columns_to_keep]

            # Save the filtered data to "cleaned4.json"
            with open('cleaned4.json', 'a') as cleaned_file:
                data_chunk.to_json(cleaned_file, orient='records', lines=True)

        # If there is no more data, break the loop
        if not chunk_has_data:
            break

        # Update the processed chunk count
        processed_chunks += 1

print("Filtered data has been written to cleaned4.json")

