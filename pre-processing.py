# -*- coding: utf-8 -*-
"""Unstrcutured.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hsN-NJUhLDox6smTpGSRT4AqoxmvDAVp
"""

import pandas as pd
import json
import re

# Define a function to remove HTML tags from text
def remove_html_tags(text):
    if isinstance(text, str):
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    elif isinstance(text, list):
        return [remove_html_tags(item) for item in text]
    else:
        return text

# Processed chunk count
# Processed chunk count
processed_chunks = 0

# Load JSON file in chunks
chunk_size = 10000  # Adjust chunk size as needed
with open('Sample_15.json', 'r') as file:
    for _ in range(processed_chunks * chunk_size):  # Skip rows if necessary
        next(file)

    for chunk in pd.read_json(file, lines=True, chunksize=chunk_size):
        # Remove unwanted columns
        unwanted_columns = ['imageURL', 'imageURL']
        chunk = chunk.drop(columns=unwanted_columns, errors='ignore')

        # Remove rows with null values
        chunk = chunk.dropna()

        # Remove HTML code from all fields
        for col in chunk.columns:
            chunk[col] = chunk[col].apply(remove_html_tags)

        # Remove rows containing "https"
        chunk = chunk[~chunk.apply(lambda row: row.astype(str).str.contains('https').any(), axis=1)]

        # Update processed chunk count
        processed_chunks += 1

        # Perform your data processing here
        #print(chunk.head())  # For demonstration purposes only, remove this line in actual processing

        # Write cleaned chunk to file
        with open('cleaned.json', 'a') as cleaned_file:
            chunk.to_json(cleaned_file, orient='records', lines=True)

chunk_size = 1000  # Adjust chunk size as needed
for chunk in pd.read_json('cleaned.json', lines=True, chunksize=chunk_size):
    # Display the head of the chunk
    print(chunk.head())

import pandas as pd

# Load previously cleaned data from "cleaned.json"
cleaned_data = pd.read_json('cleaned.json', lines=True)

# Drop columns with null values
cleaned_data = cleaned_data.dropna(axis=1)

# Drop rows with null values
cleaned_data = cleaned_data.dropna()

# Save further cleaned data to "cleaned2.json"
cleaned_data.to_json('cleaned2.json', orient='records', lines=True)

import pandas as pd

# Load previously cleaned data from "cleaned2.json"
cleaned_data = pd.read_json('cleaned2.json', lines=True)

# Remove empty strings and save further cleaned data to "cleaned3.json"
cleaned_data = cleaned_data.applymap(lambda x: None if isinstance(x, str) and x.strip() == '' else x)

# Remove '\n' from each cell and drop the column if it contains '\n' for any row
for col in cleaned_data.columns:
    cleaned_data[col] = cleaned_data[col].apply(lambda x: None if isinstance(x, str) and '\n' in x else x)

# Save further cleaned data to "cleaned4.json"
cleaned_data.to_json('cleaned3.json', orient='records', lines=True)

# Get column names after cleaning
column_names = cleaned_data.columns.tolist()
print("Column names after cleaning:", column_names)

import pandas as pd

# Load data from "cleaned4.json"
cleaned_data = pd.read_json('cleaned3.json', lines=True)

# Drop 'Tech2', 'tech2', 'image', and 'fit' columns
columns_to_drop = ['tech1', 'tech2', 'image', 'fit','date']
cleaned_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')

# Save the data to "cleaned5.json"
cleaned_data.to_json('cleaned4.json', orient='records', lines=True)

import pandas as pd

# Load data from "cleaned4.json"
cleaned_data = pd.read_json('cleaned4.json', lines=True)

# Get the column names
column_names = cleaned_data.columns.tolist()
print("Column names:", column_names)

import pandas as pd

# Load data from "cleaned4.json"
cleaned_data = pd.read_json('cleaned4.json', lines=True)

# Drop all columns except 'also_buy' and 'asin'
columns_to_keep = ['also_buy', 'asin']
cleaned_data = cleaned_data[columns_to_keep]

# Save the data to "cleaned6.json"
cleaned_data.to_json('cleaned6.json', orient='records', lines=True)

print("Filtered data has been written to cleaned6.json")
